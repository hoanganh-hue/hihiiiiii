#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Performance Optimizer cho module CCCD
T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn
"""

import time
import threading
import multiprocessing
from typing import List, Dict, Any, Optional, Callable
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import queue
import gc

class PerformanceOptimizer:
    """Class t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t cho x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn"""
    
    def __init__(self, max_workers: int = None):
        """
        Kh·ªüi t·∫°o Performance Optimizer
        
        Args:
            max_workers: S·ªë worker t·ªëi ƒëa (m·∫∑c ƒë·ªãnh = CPU count)
        """
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.thread_pool = ThreadPoolExecutor(max_workers=self.max_workers)
        self.process_pool = ProcessPoolExecutor(max_workers=self.max_workers)
        
        # Performance metrics
        self.metrics = {
            'total_processed': 0,
            'total_time': 0,
            'avg_time_per_item': 0,
            'memory_usage': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        # Cache for frequently accessed data
        self.cache = {}
        self.cache_ttl = 300  # 5 minutes
    
    def batch_process(self, 
                     items: List[Any], 
                     process_func: Callable,
                     batch_size: int = 1000,
                     use_multiprocessing: bool = False) -> List[Any]:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu theo batch ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
        
        Args:
            items: Danh s√°ch items c·∫ßn x·ª≠ l√Ω
            process_func: H√†m x·ª≠ l√Ω
            batch_size: K√≠ch th∆∞·ªõc batch
            use_multiprocessing: S·ª≠ d·ª•ng multiprocessing thay v√¨ threading
            
        Returns:
            Danh s√°ch k·∫øt qu·∫£ ƒë√£ x·ª≠ l√Ω
        """
        start_time = time.time()
        results = []
        
        # Chia items th√†nh c√°c batch
        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]
        
        if use_multiprocessing and len(batches) > 1:
            # S·ª≠ d·ª•ng multiprocessing cho batch l·ªõn
            with ProcessPoolExecutor(max_workers=min(self.max_workers, len(batches))) as executor:
                future_to_batch = {
                    executor.submit(self._process_batch, batch, process_func): batch 
                    for batch in batches
                }
                
                for future in future_to_batch:
                    try:
                        batch_results = future.result(timeout=300)  # 5 minutes timeout
                        results.extend(batch_results)
                    except Exception as e:
                        print(f"Error processing batch: {e}")
        else:
            # S·ª≠ d·ª•ng threading cho batch nh·ªè
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_batch = {
                    executor.submit(self._process_batch, batch, process_func): batch 
                    for batch in batches
                }
                
                for future in future_to_batch:
                    try:
                        batch_results = future.result(timeout=300)
                        results.extend(batch_results)
                    except Exception as e:
                        print(f"Error processing batch: {e}")
        
        # C·∫≠p nh·∫≠t metrics
        end_time = time.time()
        self.metrics['total_processed'] += len(items)
        self.metrics['total_time'] += (end_time - start_time)
        self.metrics['avg_time_per_item'] = (
            self.metrics['total_time'] / self.metrics['total_processed'] 
            if self.metrics['total_processed'] > 0 else 0
        )
        
        return results
    
    def _process_batch(self, batch: List[Any], process_func: Callable) -> List[Any]:
        """
        X·ª≠ l√Ω m·ªôt batch items
        
        Args:
            batch: Batch items
            process_func: H√†m x·ª≠ l√Ω
            
        Returns:
            K·∫øt qu·∫£ x·ª≠ l√Ω batch
        """
        results = []
        for item in batch:
            try:
                result = process_func(item)
                results.append(result)
            except Exception as e:
                print(f"Error processing item {item}: {e}")
                results.append(None)
        
        return results
    
    def memory_optimized_generation(self, 
                                  generator_func: Callable,
                                  quantity: int,
                                  chunk_size: int = 1000) -> List[Any]:
        """
        T·∫°o d·ªØ li·ªáu v·ªõi t·ªëi ∆∞u h√≥a b·ªô nh·ªõ
        
        Args:
            generator_func: H√†m t·∫°o d·ªØ li·ªáu
            quantity: S·ªë l∆∞·ª£ng c·∫ßn t·∫°o
            chunk_size: K√≠ch th∆∞·ªõc chunk
            
        Returns:
            Danh s√°ch d·ªØ li·ªáu ƒë√£ t·∫°o
        """
        results = []
        
        for i in range(0, quantity, chunk_size):
            chunk_quantity = min(chunk_size, quantity - i)
            
            # T·∫°o chunk
            chunk_results = generator_func(chunk_quantity)
            results.extend(chunk_results)
            
            # Force garbage collection sau m·ªói chunk
            gc.collect()
            
            # Log progress
            if (i + chunk_quantity) % (chunk_size * 10) == 0:
                print(f"Generated {i + chunk_quantity}/{quantity} items")
        
        return results
    
    def cache_get(self, key: str) -> Optional[Any]:
        """
        L·∫•y d·ªØ li·ªáu t·ª´ cache
        
        Args:
            key: Cache key
            
        Returns:
            Cached data ho·∫∑c None
        """
        if key in self.cache:
            data, timestamp = self.cache[key]
            if time.time() - timestamp < self.cache_ttl:
                self.metrics['cache_hits'] += 1
                return data
            else:
                # Cache expired
                del self.cache[key]
        
        self.metrics['cache_misses'] += 1
        return None
    
    def cache_set(self, key: str, data: Any) -> None:
        """
        L∆∞u d·ªØ li·ªáu v√†o cache
        
        Args:
            key: Cache key
            data: Data to cache
        """
        self.cache[key] = (data, time.time())
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """
        L·∫•y metrics hi·ªáu su·∫•t
        
        Returns:
            Dict ch·ª©a metrics
        """
        return {
            'total_processed': self.metrics['total_processed'],
            'total_time': self.metrics['total_time'],
            'avg_time_per_item': self.metrics['avg_time_per_item'],
            'items_per_second': (
                self.metrics['total_processed'] / self.metrics['total_time'] 
                if self.metrics['total_time'] > 0 else 0
            ),
            'cache_hit_rate': (
                self.metrics['cache_hits'] / (self.metrics['cache_hits'] + self.metrics['cache_misses']) * 100
                if (self.metrics['cache_hits'] + self.metrics['cache_misses']) > 0 else 0
            ),
            'cache_size': len(self.cache),
            'max_workers': self.max_workers
        }
    
    def optimize_for_quantity(self, quantity: int) -> Dict[str, Any]:
        """
        T·ªëi ∆∞u h√≥a c·∫•u h√¨nh d·ª±a tr√™n s·ªë l∆∞·ª£ng
        
        Args:
            quantity: S·ªë l∆∞·ª£ng items c·∫ßn x·ª≠ l√Ω
            
        Returns:
            Dict ch·ª©a c·∫•u h√¨nh t·ªëi ∆∞u
        """
        if quantity <= 100:
            return {
                'batch_size': 50,
                'use_multiprocessing': False,
                'chunk_size': 100,
                'max_workers': 2
            }
        elif quantity <= 1000:
            return {
                'batch_size': 200,
                'use_multiprocessing': False,
                'chunk_size': 500,
                'max_workers': 4
            }
        elif quantity <= 10000:
            return {
                'batch_size': 1000,
                'use_multiprocessing': True,
                'chunk_size': 2000,
                'max_workers': 8
            }
        else:
            return {
                'batch_size': 2000,
                'use_multiprocessing': True,
                'chunk_size': 5000,
                'max_workers': self.max_workers
            }
    
    def cleanup(self):
        """D·ªçn d·∫πp resources"""
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)
        self.cache.clear()
        gc.collect()


class CCCDPerformanceOptimizer(PerformanceOptimizer):
    """Performance Optimizer chuy√™n bi·ªát cho CCCD"""
    
    def __init__(self):
        super().__init__()
        self.province_cache = {}
        self.gender_cache = {}
    
    def optimize_cccd_generation(self, 
                               quantity: int,
                               province_codes: List[str],
                               gender: Optional[str] = None,
                               birth_year_range: Optional[tuple] = None) -> Dict[str, Any]:
        """
        T·ªëi ∆∞u h√≥a cho vi·ªác t·∫°o CCCD
        
        Args:
            quantity: S·ªë l∆∞·ª£ng CCCD c·∫ßn t·∫°o
            province_codes: Danh s√°ch m√£ t·ªânh
            gender: Gi·ªõi t√≠nh
            birth_year_range: Kho·∫£ng nƒÉm sinh
            
        Returns:
            C·∫•u h√¨nh t·ªëi ∆∞u
        """
        config = self.optimize_for_quantity(quantity)
        
        # T·ªëi ∆∞u h√≥a d·ª±a tr√™n s·ªë l∆∞·ª£ng t·ªânh
        if len(province_codes) > 10:
            config['batch_size'] = min(config['batch_size'], 500)
        
        # T·ªëi ∆∞u h√≥a d·ª±a tr√™n kho·∫£ng nƒÉm sinh
        if birth_year_range:
            year_range = birth_year_range[1] - birth_year_range[0]
            if year_range > 50:
                config['chunk_size'] = min(config['chunk_size'], 1000)
        
        return config
    
    def preload_province_data(self):
        """Preload d·ªØ li·ªáu t·ªânh/th√†nh ph·ªë"""
        try:
            from cccd.province_data import ProvinceData
            self.cache_set('provinces', ProvinceData.get_all_provinces())
            self.cache_set('province_codes', ProvinceData.get_province_codes())
            self.cache_set('province_names', ProvinceData.get_province_names())
        except ImportError:
            pass
    
    def get_optimized_batch_size(self, quantity: int) -> int:
        """
        L·∫•y batch size t·ªëi ∆∞u cho s·ªë l∆∞·ª£ng
        
        Args:
            quantity: S·ªë l∆∞·ª£ng
            
        Returns:
            Batch size t·ªëi ∆∞u
        """
        if quantity <= 100:
            return 50
        elif quantity <= 1000:
            return 200
        elif quantity <= 10000:
            return 1000
        else:
            return 2000


# Test functions
if __name__ == "__main__":
    # Test Performance Optimizer
    optimizer = CCCDPerformanceOptimizer()
    
    print("üß™ Test Performance Optimizer")
    print("=" * 50)
    
    # Test optimization configs
    test_quantities = [50, 500, 5000, 50000]
    
    for quantity in test_quantities:
        config = optimizer.optimize_cccd_generation(
            quantity=quantity,
            province_codes=["001", "079"],
            gender="Nam",
            birth_year_range=(1990, 2000)
        )
        
        print(f"Quantity: {quantity:,}")
        print(f"  Batch size: {config['batch_size']}")
        print(f"  Use multiprocessing: {config['use_multiprocessing']}")
        print(f"  Chunk size: {config['chunk_size']}")
        print(f"  Max workers: {config['max_workers']}")
        print()
    
    # Test metrics
    metrics = optimizer.get_performance_metrics()
    print("Performance Metrics:")
    for key, value in metrics.items():
        print(f"  {key}: {value}")
    
    # Cleanup
    optimizer.cleanup()
    print("\n‚úÖ Performance Optimizer test completed!")